
@inproceedings{taigman_deepface_2014,
	address = {Columbus, OH, USA},
	title = {{DeepFace}: {Closing} the {Gap} to {Human}-{Level} {Performance} in {Face} {Verification}},
	isbn = {978-1-4799-5118-5},
	shorttitle = {{DeepFace}},
	url = {https://ieeexplore.ieee.org/document/6909616},
	doi = {10.1109/CVPR.2014.220},
	abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ⇒ align ⇒ represent ⇒ classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise afﬁne transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4,000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classiﬁer. Our method reaches an accuracy of 97.35\% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27\%, closely approaching human-level performance.},
	language = {en},
	urldate = {2022-03-15},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
	month = jun,
	year = {2014},
	keywords = {Read},
	pages = {1701--1708},
	file = {Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performan.pdf:C\:\\Users\\rosco\\Zotero\\storage\\6GNZZCEN\\Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performan.pdf:application/pdf},
}

@inproceedings{parkhi_deep_2015,
	address = {Swansea},
	title = {Deep {Face} {Recognition}},
	isbn = {978-1-901725-53-7},
	url = {http://www.bmva.org/bmvc/2015/papers/paper041/index.html},
	doi = {10.5244/C.29.41},
	abstract = {The goal of this paper is face recognition – from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets.},
	language = {en},
	urldate = {2022-03-15},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2015},
	publisher = {British Machine Vision Association},
	author = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew},
	year = {2015},
	keywords = {Read},
	pages = {41.1--41.12},
	file = {Parkhi et al. - 2015 - Deep Face Recognition.pdf:C\:\\Users\\rosco\\Zotero\\storage\\H6L5558C\\Parkhi et al. - 2015 - Deep Face Recognition.pdf:application/pdf},
}

@article{koch_siamese_nodate,
	title = {Siamese {Neural} {Networks} for {One}-shot {Image} {Recognition}},
	abstract = {The process of learning good features for machine learning applications can be very computationally expensive and may prove difﬁcult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classiﬁcation tasks.},
	language = {en},
	author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
	keywords = {Read},
	pages = {8},
	file = {Koch et al. - Siamese Neural Networks for One-shot Image Recogni.pdf:C\:\\Users\\rosco\\Zotero\\storage\\2MPLLBTC\\Koch et al. - Siamese Neural Networks for One-shot Image Recogni.pdf:application/pdf},
}

@inproceedings{viola_rapid_2001,
	title = {Rapid object detection using a boosted cascade of simple features},
	volume = {1},
	doi = {10.1109/CVPR.2001.990517},
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	booktitle = {Proceedings of the 2001 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}. {CVPR} 2001},
	author = {Viola, P. and Jones, M.},
	month = dec,
	year = {2001},
	note = {ISSN: 1063-6919},
	keywords = {Detectors, Object detection, Face detection, Filters, Focusing, Image representation, Machine learning, Pixel, Robustness, Skin, NOT read},
	pages = {I--I},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\rosco\\Zotero\\storage\\BT8FVF6V\\Viola and Jones - 2001 - Rapid object detection using a boosted cascade of .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\rosco\\Zotero\\storage\\CM4FLM79\\990517.html:text/html},
}

@article{bajic_chart_2021,
	title = {Chart {Classification} {Using} {Siamese} {CNN}},
	volume = {7},
	issn = {2313-433X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8622233/},
	doi = {10.3390/jimaging7110220},
	abstract = {In recovering information from the chart image, the first step should be chart type classification. Throughout history, many approaches have been used, and some of them achieve results better than others. The latest articles are using a Support Vector Machine (SVM) in combination with a Convolutional Neural Network (CNN), which achieve almost perfect results with the datasets of few thousand images per class. The datasets containing chart images are primarily synthetic and lack real-world examples. To overcome the problem of small datasets, to our knowledge, this is the first report of using Siamese CNN architecture for chart type classification. Multiple network architectures are tested, and the results of different dataset sizes are compared. The network verification is conducted using Few-shot learning (FSL). Many of described advantages of Siamese CNNs are shown in examples. In the end, we show that the Siamese CNN can work with one image per class, and a 100\% average classification accuracy is achieved with 50 images per class, where the CNN achieves only average classification accuracy of 43\% for the same dataset.},
	number = {11},
	urldate = {2022-03-24},
	journal = {Journal of Imaging},
	author = {Bajić, Filip and Job, Josip},
	month = oct,
	year = {2021},
	pmid = {34821851},
	pmcid = {PMC8622233},
	keywords = {Read},
	pages = {220},
	file = {Full Text:C\:\\Users\\rosco\\Zotero\\storage\\PRCSD822\\Bajić and Job - 2021 - Chart Classification Using Siamese CNN.pdf:application/pdf},
}

@article{song_occlusion_2019,
	title = {Occlusion {Robust} {Face} {Recognition} {Based} on {Mask} {Learning} with {PairwiseDifferential} {Siamese} {Network}},
	url = {http://arxiv.org/abs/1908.06290},
	abstract = {Deep Convolutional Neural Networks (CNNs) have been pushing the frontier of the face recognition research in the past years. However, existing general CNN face models generalize poorly to the scenario of occlusions on variable facial areas. Inspired by the fact that a human visual system explicitly ignores occlusions and only focuses on nonoccluded facial areas, we propose a mask learning strategy to ﬁnd and discard the corrupted feature elements for face recognition. A mask dictionary is ﬁrstly established by exploiting the differences between the top convoluted features of occluded and occlusion-free face pairs using an innovatively designed Pairwise Differential Siamese Network (PDSN). Each item of this dictionary captures the correspondence between occluded facial areas and corrupted feature elements, which is named Feature Discarding Mask (FDM). When dealing with a face image with random partial occlusions, we generate its FDM by combining relevant dictionary items and then multiply it with the original features to eliminate those corrupted feature elements. Comprehensive experiments on both synthesized and realistic occluded face datasets show that the proposed approach signiﬁcantly outperforms the state-of-the-arts.},
	language = {en},
	urldate = {2022-04-14},
	journal = {arXiv:1908.06290 [cs]},
	author = {Song, Lingxue and Gong, Dihong and Li, Zhifeng and Liu, Changsong and Liu, Wei},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.06290},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, NOT read},
	file = {Song et al. - 2019 - Occlusion Robust Face Recognition Based on Mask Le.pdf:C\:\\Users\\rosco\\Zotero\\storage\\BHUT4YDM\\Song et al. - 2019 - Occlusion Robust Face Recognition Based on Mask Le.pdf:application/pdf},
}

@article{dey_signet_2017,
	title = {{SigNet}: {Convolutional} {Siamese} {Network} for {Writer} {Independent} {Offline} {Signature} {Verification}},
	shorttitle = {{SigNet}},
	url = {http://arxiv.org/abs/1707.02131},
	abstract = {Oﬄine signature veriﬁcation is one of the most challenging tasks in biometrics and document forensics. Unlike other veriﬁcation problems, it needs to model minute but critical details between genuine and forged signatures, because a skilled falsiﬁcation might only diﬀer from a real signature by some speciﬁc kinds of deformation. This veriﬁcation task is even harder in writer independent scenarios which is undeniably ﬁscal for realistic cases. In this paper, we model an oﬄine writer independent signature veriﬁcation task with a convolutional Siamese network. Siamese networks are twin networks with shared weights, which can be trained to learn a feature space where similar observations are placed in proximity. This is achieved by exposing the network to a pair of similar and dissimilar observations and minimizing the Euclidean distance between similar pairs while simultaneously maximizing it between dissimilar pairs. Experiments conducted on cross-domain datasets emphasize the capability of our network to handle forgery in diﬀerent languages (scripts) and handwriting styles. Moreover, our designed Siamese network, named SigNet, provided better results than the state-of-the-art results on most of the benchmark signature datasets.},
	language = {en},
	urldate = {2022-04-20},
	journal = {arXiv:1707.02131 [cs]},
	author = {Dey, Sounak and Dutta, Anjan and Toledo, J. Ignacio and Ghosh, Suman K. and Llados, Josep and Pal, Umapada},
	month = sep,
	year = {2017},
	note = {arXiv: 1707.02131},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Read},
	file = {Dey et al. - 2017 - SigNet Convolutional Siamese Network for Writer I.pdf:C\:\\Users\\rosco\\Zotero\\storage\\8LEQITJ5\\Dey et al. - 2017 - SigNet Convolutional Siamese Network for Writer I.pdf:application/pdf},
}

@inproceedings{hadsell_dimensionality_2006,
	title = {Dimensionality {Reduction} by {Learning} an {Invariant} {Mapping}},
	volume = {2},
	doi = {10.1109/CVPR.2006.100},
	abstract = {Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.},
	booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'06)},
	author = {Hadsell, R. and Chopra, S. and LeCun, Y.},
	month = jun,
	year = {2006},
	note = {ISSN: 1063-6919},
	keywords = {Read, Feature extraction, Astronomy, Biology, Data visualization, Extraterrestrial measurements, Geoscience, Image analysis, Image generation, Manufacturing industries, Service robots},
	pages = {1735--1742},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\rosco\\Zotero\\storage\\8PEZDW8B\\Hadsell et al. - 2006 - Dimensionality Reduction by Learning an Invariant .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\rosco\\Zotero\\storage\\QBV8HFWX\\1640964.html:text/html},
}

@article{otoole_face_2007,
	title = {Face {Recognition} {Algorithms} {Surpass} {Humans} {Matching} {Faces} {Over} {Changes} in {Illumination}},
	volume = {29},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2007.1107},
	abstract = {There has been significant progress in improving the performance of computer-based face recognition algorithms over the last decade. Although algorithms have been tested and compared extensively with each other, there has been remarkably little work comparing the accuracy of computer-based face recognition systems with humans. We compared seven state-of-the-art face recognition algorithms with humans on a facematching task. Humans and algorithms determined whether pairs of face images, taken under different illumination conditions, were pictures of the same person or of different people. Three algorithms surpassed human performance matching face pairs prescreened to be "difficult" and six algorithms surpassed humans on "easy" face pairs. Although illumination variation continues to challenge face recognition algorithms, current algorithms compete favorably with humans. The superior performance of the best algorithms over humans, in light of the absolute performance levels of the algorithms, underscores the need to compare algorithms with the best current control—humans.},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {O'Toole, Alice J. and Jonathon Phillips, P. and Jiang, Fang and Ayyad, Janet and Penard, Nils and Abdi, Herve},
	month = sep,
	year = {2007},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Information processing, Application software, Computer security, Computer vision, face and gesture recognition, Face recognition, human information processing, Humans, Information security, Lighting control, performance evaluation of algorithms and systems, System testing, US Government},
	pages = {1642--1646},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\rosco\\Zotero\\storage\\PUFY6D42\\O'Toole et al. - 2007 - Face Recognition Algorithms Surpass Humans Matchin.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\rosco\\Zotero\\storage\\Y4E9J64N\\4288163.html:text/html},
}

@article{otoole_predicting_nodate,
	title = {Predicting {Human} {Performance} for {Face} {Recognition}},
	abstract = {The ability of humans to recognize faces provides an implicit benchmark for gauging the performance of automatic face recognition algorithms. In this chapter we review the factors that affect human accuracy. These factors can be classiﬁed into facial stucture constraints and viewing parameters. The former include factors such as face typicality, gender, and ethnicity. The latter include changes in illumination and viewpoint, as well as the perceptual complications introduced when we see faces and people in motion. The common thread of the chapter is that human experience and familiarity with faces can overcome many, if not all, of these challenges to face recognition. A goal of computional algorithms should be to emulate the ways in which humans acquire familiarity with faces. It may then be possible to apply these principles to the design of algorithms to meet the pressing challenges of face recognition in naturalistic vieiwng conditions.},
	language = {en},
	author = {O’Toole, Alice J and Jiang, Fang and Roark, Dana and Abdi, Hervé},
	keywords = {Read},
	pages = {27},
	file = {O’Toole et al. - Predicting Human Performance for Face Recognition.pdf:C\:\\Users\\rosco\\Zotero\\storage\\DFJP367P\\O’Toole et al. - Predicting Human Performance for Face Recognition.pdf:application/pdf},
}

@article{altenberger_non-technical_2018,
	title = {A {Non}-{Technical} {Survey} on {Deep} {Convolutional} {Neural} {Network} {Architectures}},
	url = {http://arxiv.org/abs/1803.02129},
	abstract = {Artiﬁcial neural networks have recently shown great results in many disciplines and a variety of applications, including natural language understanding, speech processing, games and image data generation. One particular application in which the strong performance of artiﬁcial neural networks was demonstrated is the recognition of objects in images, where deep convolutional neural networks are commonly applied. In this survey, we give a comprehensive introduction to this topic (object recognition with deep convolutional neural networks), with a strong focus on the evolution of network architectures. Therefore, we aim to compress the most important concepts in this ﬁeld in a simple and non-technical manner to allow for future researchers to have a quick general understanding.},
	language = {en},
	urldate = {2022-05-06},
	journal = {arXiv:1803.02129 [cs]},
	author = {Altenberger, Felix and Lenz, Claus},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.02129},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 17 pages (incl. references), 23 Postscript figures, uses IEEEtran},
	file = {Altenberger and Lenz - 2018 - A Non-Technical Survey on Deep Convolutional Neura.pdf:C\:\\Users\\rosco\\Zotero\\storage\\DGGTUZUU\\Altenberger and Lenz - 2018 - A Non-Technical Survey on Deep Convolutional Neura.pdf:application/pdf},
}

@article{huang_learning_nodate,
	title = {Learning to {Align} from {Scratch}},
	abstract = {Unsupervised joint alignment of images has been demonstrated to improve performance on recognition tasks such as face veriﬁcation. Such alignment reduces undesired variability due to factors such as pose, while only requiring weak supervision in the form of poorly aligned examples. However, prior work on unsupervised alignment of complex, real-world images has required the careful selection of feature representation based on hand-crafted image descriptors, in order to achieve an appropriate, smooth optimization landscape. In this paper, we instead propose a novel combination of unsupervised joint alignment with unsupervised feature learning. Speciﬁcally, we incorporate deep learning into the congealing alignment framework. Through deep learning, we obtain features that can represent the image at differing resolutions based on network depth, and that are tuned to the statistics of the speciﬁc data being aligned. In addition, we modify the learning algorithm for the restricted Boltzmann machine by incorporating a group sparsity penalty, leading to a topographic organization of the learned ﬁlters and improving subsequent alignment results. We apply our method to the Labeled Faces in the Wild database (LFW). Using the aligned images produced by our proposed unsupervised algorithm, we achieve higher accuracy in face veriﬁcation compared to prior work in both unsupervised and supervised alignment. We also match the accuracy for the best available commercial method.},
	language = {en},
	author = {Huang, Gary B and Mattar, Marwan A and Lee, Honglak and Learned-Miller, Erik},
	pages = {9},
	file = {Huang et al. - Learning to Align from Scratch.pdf:C\:\\Users\\rosco\\Zotero\\storage\\UPUTJER9\\Huang et al. - Learning to Align from Scratch.pdf:application/pdf},
}

@article{huang_labeled_nodate,
	title = {Labeled {Faces} in the {Wild}: {A} {Database} for {Studying} {Face} {Recognition} in {Unconstrained} {Environments}},
	abstract = {Face recognition has beneﬁtted greatly from the many databases that have been produced to study it. Most of these databases have been created under controlled conditions to facilitate the study of speciﬁc parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, expression, background, camera quality, occlusion, age, and gender.},
	language = {en},
	author = {Huang, Gary B and Ramesh, Manu and Berg, Tamara and Learned-Miller, Erik},
	pages = {11},
	file = {Huang et al. - Labeled Faces in the Wild A Database for Studying.pdf:C\:\\Users\\rosco\\Zotero\\storage\\GQDPCM3D\\Huang et al. - Labeled Faces in the Wild A Database for Studying.pdf:application/pdf},
}
